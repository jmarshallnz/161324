---
title: 'Lecture 9'
subtitle: 'K-nearest neighbours, Trees, Forests and Neural Nets'
output:
  xaringan::moon_reader:
    css: [default, default-fonts, "custom.css"]
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
    nature:
      navigation:
        scroll: false
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);" />
        </div>
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(patchwork)
library(skimr)
library(rsample)

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(4.8, 4.5), fig.retina=2, out.width="100%", comment="#>")
theme_set(theme_minimal())

swiss.train <- read_csv("../data/swiss-train.csv") |>
  mutate(type = factor(type))
swiss.test <- read_csv("../data/swiss-test.csv") |>
  mutate(type = factor(type))

wine.train <- read_csv("../data/wine-train.csv") |>
  mutate(Cultivar = factor(Cultivar))
wine.test <- read_csv("../data/wine-test.csv") |>
  mutate(Cultivar = factor(Cultivar))

donut.train <- read_csv("../data/donut-train.csv") |>
  mutate(Class = factor(Class))

covid_all <- read_csv("https://www.massey.ac.nz/~jcmarsha/data/covid19/symptoms.csv") |>
  mutate(across(everything(), as.factor), Status = fct_relevel(Status, "Not_a_case"))

set.seed(3)
split <- initial_split(covid_all, prop=3/4)

covid.train <- training(split)
covid.test  <- testing(split)
```

## K-Nearest neighbour classification

-   Find $k$ data points in the training set that are most similar to
    the test observation at hand.

-   Assign the test case to the most common class for these proximate
    training data.

-   Similar to missing value imputation.

---

## K-Nearest neighbour classification

There are two main things to consider:

-   Scale. Euclidean distance is often used between observations to
    determine the $k$ most similar items.

-   What $k$ should be.

    - Small $k$ will give larger variation as few observations in the
    training data are used to predict. But, those observations will
    be relative close to the test data so will likely have lower bias.

    - Larger $k$ will give less
    variation based on the majority vote of a large number of
    observations from the training data. However, larger $k$ will have
    bias as we will be comparing the test data to observations that have
    greater disparity in the pattern of predictors.

---

## Breaking ties

There are two ways a tie can occur in $k$-nearest
neighbour classification.

-   Ties between the distances (i.e. more than $k$ observations in the
    training set have the same distance as the nearest $k$ points from
    the test observation). In this case we normally use all observations
    within the same distance as the first $k$. An alternate would be to
    pick the $k$ neighbours at random.

-   A tie in the majority vote of the $k$ nearest observations for
    class. In this case we normally assign the class randomly out of the
    top voted classes.

---

## K-Nearest neighbours in R

We will use the `tidymodels` framework, utilising the specification

```{r, eval=FALSE}
nearest_neighbor(mode = 'classification', neighbors = 5)
```

By default this uses the `kknn` package, which automatically scales
the predictor variables to have common variance.

Thus, we need only concern ourselves with determining $k$.

---

## K-Nearest neighbours for the Wine data

```{r, message=FALSE}
library(parsnip)
library(yardstick)
wine.knn1 <- nearest_neighbor(mode="classification", neighbors = 1) |>
  fit(Cultivar ~ ., data=wine.train)
wine.knn1.pred <- wine.knn1 |>
  augment(new_data = wine.test)
wine.knn1.pred |>
  conf_mat(truth=Cultivar, estimate=.pred_class)
```

With $k=1$ we find 9 wines get misclassified - all of Cultivar B.

---

## K-Nearest neighbours for the Wine data

```{r}
wine.knn2 <- nearest_neighbor(mode="classification", neighbors = 20) |>
  fit(Cultivar ~ ., data=wine.train)
wine.knn2.pred <- wine.knn2 |>
  augment(new_data = wine.test)
wine.knn2.pred |>
  conf_mat(truth=Cultivar, estimate=.pred_class)
```

With $k=20$ it is quite a bit better - now just 4 wines get misclassified, again Cultivar B.

---

class: middle, inverse

# Classification Trees

---

## Classification Trees

-   Similar to regression trees.

-   Instead of the mean prediction at each node, we use the majority
    vote of class at each node.

-   Instead of minimising RSS to find the next optimal split, we instead
    minimise a measure of node impurity.

---

## Node purity

-   Purity measures how close a node is to being observations of just
    one class.

-   Impurity is then a measure of how close the node is to having
    observations equally distributed among all classes.

-   We can measure impurity using the Gini splitting index, which measures the chance an
    observation from the node would be misclassified if all the class
    labels were randomly reallocated within that node.

---

## Gini splitting index

Let $\hat{p}_j$ be the proportion of observations
at a node in class $j$, and let $x$ be an observation at that node. If
we randomly reallocate observations to classes within the node,
$$\begin{aligned} P(x \textsf{ misclassified}) &= \sum_{j=1}^C P(x \textsf{ misclassified}|x \in j)P(x \in j)\\ &= \sum_{j=1}^C (1-\hat{p}_j)\hat{p}_j\\
&= 1 - \sum_{j=1}^C \hat{p}_j^2.\end{aligned}$$

We try to find splits that minimise the Gini index.

---

## Gini splitting index

$$P(x \textsf{ misclassified}) = 1 - \sum_{j=1}^C \hat{p}_j^2.$$

-   This is smallest when $\hat{p}_c = 1$ for some $c\in C$ and
    $\hat{p}_j = 0$ for $j \neq c$.

-   This is largest when $\hat{p}_j = \frac{1}{C}$ for all $j$.

-   To find the best split, work out the Gini index $g$ at the pair of
    nodes $l,r$ formed by each split and then minimise their sum,
    weighted by the proportion $p$ of observations going to each node,
    $$p(l)g(l) + p(r)g(r).$$

---

## Finding the best split
.left-code[
Consider the data

| x | class |
| --: | --- |
| 2.8 |  a  |
| 3.0 |  b  |
| 3.2 |  a  |
| 3.4 |  b  |
| 3.6 |  b  |

There are 4 possibilities for the first split.

Which is best?
]

--

.right-plot[

- $x=2.9$ gives a pure left node, and a right node with impurity $1-\left[\frac{1}{4}\right]^2-\left[\frac{3}{4}\right]^2 = \frac{3}{8}$. Total impurity is $\frac{1}{5}\times 0 + \frac{4}{5}\times\frac{3}{8} = \frac{3}{10}$.

- $x=3.1$ gives a left node with impurity $\frac{1}{2}$ and a right node with impurity $\frac{4}{9}$, giving total impurity $\frac{2}{5}\times\frac{1}{2} + \frac{3}{5}\times\frac{4}{9} = \frac{7}{15}.$

- $x=3.3$ gives left node impurity $\frac{4}{9}$ and a pure right node, giving total impurity $\frac{3}{5}\times\frac{4}{9} + \frac{2}{5}\times0 = \frac{4}{15}.$

- $x=3.5$ gives left node impurity $\frac{1}{2}$ and a pure right node, giving total impurity $\frac{4}{5}\times\frac{1}{2} + \frac{1}{5}\times0 = \frac{2}{5}.$

So $x=3.3$ is the best split.
]

---

.left-code[
## Classification trees for Swiss bank notes

```{r}
swiss.rp <- decision_tree(
  mode = 'classification'
  ) |>
  fit(type ~ ., data=swiss.train)

swiss.rp.pred <- swiss.rp |>
  augment(swiss.test)

swiss.rp.pred |>
  conf_mat(truth=type,
           estimate=.pred_class)
```
]

.right-plot[
```{r, echo=FALSE, message=FALSE}
swiss.rp |>
  extract_fit_engine() |>
  plot(margin=0.1)
swiss.rp |>
  extract_fit_engine() |>
  text()
```
]

---

.left-code[
## Classification tree for Swiss bank notes

```{r}
swiss.rp2 <- decision_tree(
  mode='classification',
  min_n=7
  ) |>
  fit(type ~ ., data=swiss.train)

swiss.rp2.pred <- swiss.rp2 |>
  augment(swiss.test)

swiss.rp2.pred |>
  conf_mat(truth=type,
           estimate=.pred_class)
```
]

.right-plot[
```{r, echo=FALSE, message=FALSE}
swiss.rp2 |> extract_fit_engine() |> plot(margin=0.1)
swiss.rp2 |> extract_fit_engine() |> text()
```
]

---

.left-code[
## Classification tree for Swiss bank notes

```{r}
swiss.rp2 <- decision_tree(
  mode='classification',
  min_n=7
  ) |>
  fit(type ~ ., data=swiss.train)

swiss.rp2.pred <- swiss.rp2 |>
  augment(swiss.test)

swiss.rp2.pred |>
  conf_mat(truth=type,
           estimate=.pred_class)
```
]

.right-plot[
```{r, echo=FALSE, message=FALSE}
swiss.all <- bind_rows(list(test=swiss.test, train=swiss.train), .id='data')
swiss.grid <- expand_grid(margin=seq(7.1,12.8,by=0.05),
                          diagonal=seq(137.5, 142.5, by=0.05))

ggplot(swiss.all) +
  geom_tile(data=swiss.rp2 |> augment(swiss.grid),
            mapping=aes(x=margin, y=diagonal, fill=.pred_class), alpha=0.3) +
  geom_point(mapping=aes(x=margin, y=diagonal, col=type, alpha=data)) +
  scale_alpha_manual(values = c(test=1, train=0.3)) +
  scale_x_continuous(expand=c(0,0)) +
  scale_y_continuous(expand=c(0,0)) +
  guides(fill='none')
```
]

---

## Random forests for classification

- Forests for classification are just collections of classification trees.

- Classification is done by passing each test observation through each tree.

- This gives the most likely class from each tree.

- These are treated as votes for the most likely class for the forest.

    - Can result in ties, which are usually resolved at random.

---

class: middle, inverse

# Neural networks for classification

---

.left-code[
## Neural networks for classification

-   Neural networks attempt to model the brain by assembling a large
    number of interconnected nodes (neurons) which 'fire' or output only
    if the input is large enough.

-   Nodes are ordered into levels and all nodes on one level feed into
    the next level.

    -   Input nodes are the input variables.

    -   These feed into one or more hidden layers or derived features.

    -   The derived features feed into the output node(s).
]

.right-plot[
```{r, echo=FALSE}
source("../common/plots.R")
nnet_plot()
```
]

---

## Neural networks for classification

-   A linear combination of nodes at the previous layer feed through an activation function
    $\phi$ to nodes in the current layer.

-   The default activation function is the sigmoid, also used for
    logistic/multinomial regression
    $$\phi(\theta) = \frac{1}{1+e^{-\theta}}.$$

-   This gives the 'firing' style behaviour as $\theta$ moves through
    zero.

---

## Neural networks for classification

-   We need to define

    -   The number of hidden layers.

    -   The number of derived features in the hidden layers.

    -   The activation functions from one layer to the next.

-   There is little theory to assist us with these, but some simple
    criteria are

    -   Use a single hidden layer.

    -   Use $\lceil p/2\rceil$ derived features, where $p$ is the
        effective number of predictors.

    -   The sigmoid activation function is usually used.

---

## Neural networks for classification

-   The derived features $z_k$ are defined in terms of the predictors
    $x_p$ by
    $$z_k = \phi(\alpha_{0k} + \alpha_{1k}x_1 + \cdots + \alpha_{pk}x_p).$$

-   The outcome nodes for classification are the probability of a class,
    so are also defined using the sigmoid function
    $$\hat{p_c} = \phi(\beta_{0c} + \beta_{1c}z_1 + \cdots + \beta_{kc}z_k).$$

-   The $\alpha$ and $\beta$ terms (the *weights*) are then estimated from
    training data.

---

## Fitting neural networks for classification in R

We'll utilise the `mlp()` specification from the `tidymodels` framework:

```{r, eval=FALSE}
mlp(mode='classification', hidden_units=2, penalty=0.1, epochs=1000)
```

-   The `hidden_units` is the number of hidden nodes.

-   `penalty` is the decay rate for the back propagation algorithm.

-   `epochs` is the number of iterations.

-   Remember that large numeric inputs should be scaled first.

---

## Neural network classification of Swiss data

```{r, message=FALSE}
library(recipes)
swiss.rec <- recipe(type ~ ., data=swiss.train) |>
  step_normalize(all_numeric_predictors()) |>
  prep(swiss.train)

set.seed(2001)
swiss.nn <- mlp(mode='classification', hidden_units=2, epochs=500) |>
  fit(type ~ ., data=bake(swiss.rec, swiss.train))

swiss.nn.pred <- swiss.nn |> augment(new_data=bake(swiss.rec, swiss.test))

swiss.nn.pred |> conf_mat(truth=type, estimate=.pred_class)
```

---

class: middle, inverse

# COVID-19 Symptoms

## Trees, forests and Neural nets

---

.left-code[
## Symptoms of COVID-19 cases in New Zealand

```{r symptoms_plot, eval=FALSE}
covid.train |>
  select(-AgeGrp, -Month, -Sex) |>
  pivot_longer(-Status) |>
  group_by(Status, name, value) |>
  summarise(n=n()) |>
  mutate(p=n/sum(n)) |>
  filter(value != "No") |>
  ggplot() +
  geom_col(
    mapping=aes(
      y=name,
      x=p
      )
    ) +
  facet_wrap(vars(Status)) +
  labs(x=NULL, y=NULL) +
  scale_x_continuous(
    labels=scales::label_percent()
  )
```
]

.right-plot[
```{r, ref.label='symptoms_plot', echo=FALSE, message=FALSE}
```
]

---

.left-code[
## Symptoms of COVID-19: Tree

```{r}
covid.rp <- decision_tree(
  mode='classification',
  cost_complexity=0.001
) |>
  fit(Status ~ ., data=covid.train)

covid.rp.pred <- 
  covid.rp |>
  augment(covid.test)

covid.rp.pred |>
  conf_mat(truth=Status,
           estimate=.pred_class)
```
]

.right-plot[
```{r, echo=FALSE, message=FALSE}
par(mar=c(0,0,0,0))
covid.rp |>
  extract_fit_engine() |>
  plot(margin=0.1)
covid.rp |>
  extract_fit_engine() |>
  text(cex=0.4)
```
]

---

## Symptoms of COVID-19: Forest

```{r}
set.seed(1234)
covid.rf <- rand_forest(mode='classification', mtry=2) |>
  fit(Status ~ ., data=covid.train)

covid.rf.pred <- covid.rf |>
  augment(covid.test)

covid.rf.pred |>
  conf_mat(truth=Status,
           estimate=.pred_class)
```

---

## Symptoms of COVID-19: Neural net

```{r}
set.seed(1234)
covid.nn <- mlp(mode='classification', hidden_units=7, penalty = 0.001, epochs=1000) |>
  fit(Status ~ ., data=covid.train)
covid.nn |> pluck('fit', 'convergence')

covid.nn.pred <- covid.nn |>
  augment(covid.test)

covid.nn.pred |>
  conf_mat(truth=Status,
           estimate=.pred_class)
```

---

```{r, echo=FALSE}
covid.lr <- logistic_reg() |>
  fit(Status ~ ., data=covid.train)
covid.lr.pred <- covid.lr |> augment(covid.test)
```

.left-code[
## Symptoms of COVID-19: ROC curves

```{r roc_curves, eval=FALSE}
all <- bind_rows(
  list(forest=covid.rf.pred,
       logistic=covid.lr.pred,
       tree=covid.rp.pred,
       nnet=covid.nn.pred),
  .id="Model")
all |>
  group_by(Model) |>
  roc_curve(
    truth=Status,
    .pred_Confirmed,
    event_level = "second",
    ) |>
  autoplot() +
  theme(legend.position='top')
```

The logistic regression is almost uniformly best.
]

.right-plot[
```{r, ref.label="roc_curves", echo=FALSE, message=FALSE}
```
]

---

## Dealing with class imbalance

- None of the models are any good at predicting confirmed cases of COVID-19.

- We could use a different threshold (like we did with the logistic) to move along
the ROC curve. But the ROC curve as it is shows logistic regression best.

- Another way to deal with class imbalance is to re-balance the classes prior to
training.

- We could *downsample* the 'not a case' group, and/or *upsample* the 'Confirmed' group.

- One method is to use SMOTE: Synthetic minority over-sampling technique.

---

## SMOTE: Over-sampling

- The idea is to generate some more observations in the under-sampled class using
$k$-nearest neighbour sampling.

- We can optionally also downsample the over-sampled class.

- This then gives better balance, which may help performance on the under-sampled class.

- The undersampled class is often the interesting one!

    - See here for more: https://jair.org/index.php/jair/article/view/10302/24590

---

## Implementing SMOTE in R

.left-code-wide[
We use `step_smotenc()` from the `themis` package which handles categorical predictors.

```{r, message=FALSE}
library(themis)
set.seed(1234)
rec <- recipe(Status ~ ., data=covid.train) |>
  step_smotenc(Status,
               over_ratio = 1,
               skip=TRUE) |>
  prep(covid.train)

covid.balanced <- rec |> bake(new_data=NULL)
covid.balanced |> count(Status)
```
]

.right-plot-narrow[
- The `over_ratio` parameter specifies the majority to minority class ratio.

- We use `skip=TRUE` so that this step applies only to the data we `prep()` with (training data).

- If we apply `bake()` with new data, the step will be skipped.

    - `new_data=NULL` ensures we get the prep'd training data back.
]
---

.left-code[
## Refitting the Forest

```{r}
set.seed(1234)
covid.rf2 <-
  rand_forest(
    mode='classification',
    mtry=2) |>
  fit(Status ~ .,
      data=covid.balanced)

covid.rf2.pred <- covid.rf2 |>
  augment(rec |> bake(covid.test))

covid.rf2.pred |>
  conf_mat(truth=Status,
           estimate=.pred_class)
```
]

.right-plot[
```{r, echo=FALSE}
both <- bind_rows(
  list(balanced=covid.rf2.pred,
       logistic=covid.lr.pred,
       unbalanced=covid.rf.pred),
  .id="Model")

both |>
  group_by(Model) |>
  roc_curve(
    truth=Status,
    .pred_Confirmed,
    event_level='second'
    ) |>
  autoplot() +
  theme(legend.position='top')
```
]

---

class: middle, inverse

# Summary

---

## Classification summary

-   Some methods require quantitative predictors:

    -   Linear discriminant analysis, kernel discriminant analysis,
        $k$-nearest neighbours.

-   Some require all predictors on the same scale.

    -   $k$-nearest neighbours. This is handled automatically by `nearest_neighbour` specification.

    -   Neural networks. Scale large predictors first.

-   Some use a linear combination of predictors.

    -   Logistic or Multinomial regression.

-   Some have strong assumptions.

    -   LDA, Naive Bayes, Logistic or Multinomial regression.

---

## Classification summary

-   Methods with the greatest number of assumptions will do well when
    those assumptions hold.

-   They can do very poorly if the assumptions do not hold.

-   In practice we don't know for certain whether the assumptions hold,
    so many data miners prefer to use more flexible tools such as
    trees, forests or neural networks.

-   If classes are imbalanced, then your 'baseline' level for predictive
    accuracy is "assign everything to the biggest group".

-   Rebalancing the classes might help in some cases, either through under or over-sampling, or a combination.
