---
title: 'Lecture 6'
subtitle: 'Neural networks and tidy models'
output:
  xaringan::moon_reader:
    css: [default, default-fonts, "custom.css"]
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
    nature:
      navigation:
        scroll: false
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);" />
        </div>
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
library(skimr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(4.8, 4.5), fig.retina=2, out.width="100%", comment="#>")
theme_set(theme_minimal())

wage.train <- read_csv("../data/wage-train.csv")
wage.test  <- read_csv("../data/wage-test.csv")
```

.left-code[
## Neural Networks

-   An artificial neural network is
    a type of prediction model based on a crude representation
    of a human brain.

-   The model comprises a highly interconnected network of
    'neurons' which 'fire' (output) only if the level of input
    is sufficiently high.
]

.right-plot[
```{r, echo=FALSE}
knitr::include_graphics("graphics/brain.jpg")
```
]

---

.left-code[
## Neural Networks

An artificial feed-forward neural network:

-   Comprises an ordered sequence of layers of neurons.

-   Each node connects to all nodes in next layer.

-   The input layer (predictor nodes) feeds to a hidden layer
    (derived features $z$), and then to an output node (target
    variable).

-   Can have multiple hidden layers (deep learning) but we'll be
    sticking with one for simplicity!
]

.right-plot[
```{r, echo=FALSE}
par(mar=c(0,0,0,0))
plot(c(1,9),c(0,11),type="n",axes=F,xlab="",ylab="")
symbols(c(2,2,2,2,5,5,5,8),c(1,3.67,6.33,9,2,5,8,5),circle=rep(0.5,8),inches=F,add=T)
arrows(2.5,1,4.5,2,length=0.1)
arrows(2.5,1,4.5,5,length=0.1)
arrows(2.5,1,4.5,8,length=0.1)
arrows(2.5,3.67,4.5,2,length=0.1)
arrows(2.5,3.67,4.5,5,length=0.1)
arrows(2.5,3.67,4.5,8,length=0.1)
arrows(2.5,6.33,4.5,2,length=0.1)
arrows(2.5,6.33,4.5,5,length=0.1)
arrows(2.5,6.33,4.5,8,length=0.1)
arrows(2.5,9,4.5,2,length=0.1)
arrows(2.5,9,4.5,5,length=0.1)
arrows(2.5,9,4.5,8,length=0.1)
arrows(5.5,2,7.5,5,length=0.1)
arrows(5.5,5,7.5,5,length=0.1)
arrows(5.5,8,7.5,5,length=0.1)
text(2,9,expression(x[1]),cex=1.5)
text(2,6.33,expression(x[2]),cex=1.5)
text(2,3.67,expression(x[3]),cex=1.5)
text(2,1,expression(x[4]),cex=1.5)
text(5,8,expression(z[1]),cex=1.5)
text(5,5,expression(z[2]),cex=1.5)
text(5,2,expression(z[3]),cex=1.5)
text(8,5,expression(hat(y)),cex=1.5)
text(2,10.2,"Inputs",cex=1.5)
text(5,10.2,"Hidden layer",cex=1.5)
text(8,10.2,"Output",cex=1.5)
```
]
---

## Building a Neural Network

-   To implement a neural network we must:

    1.  decide on the number, $M$, of nodes in the hidden layer;

    2.  devise functions to relate the *derived features*
        $z_1, \ldots, z_M$ (i.e. the values in the hidden layer) to the
        input variables;

    3.  choose a function to describe the predicted target $\hat y$ in
        terms of $z_1, \ldots, z_M$.

-   There is little theory to help with these choices.

-   Rule of thumb: try $\lceil p/2 \rceil$ hidden nodes.

    -   $\lceil p/2 \rceil$ is smallest integer at least as large as
        $p/2$; e.g. $\lceil 7/2 \rceil = 4$.

    -   $p$ is effective number of predictors, where a factor on $K$
        levels contributes $K-1$ predictors (through coding to indicators
        variables).

    -   e.g. If there are 3 numerical predictors and one factor on 4
        levels, then $p = 3+ (4-1) = 6$.
---

## Building a Neural Network

-   The function that relates the derived features to the inputs
    (i.e. predictors) is called the *activation function*.

    -   Denote by $\phi$.

-   $\phi$ operates on a linear combination of the predictors.

-   Hence $z_k = \phi(v_k)$ where
    $$v_k = \alpha_{0k} + \alpha_{1k} x_1 + \cdots + \alpha_{pk} x_p~~~~(k=1,\ldots,M)$$

-   The predicted target $\hat y$ is derived as a linear combination of the
    hidden features:
    $$\hat y = \beta_0 + \beta_1 z_1 + \cdots + \beta_M z_M.$$

-   There are $M$ linear models with $p+1$ parameters for the hidden layer which are then transformed with the non-linear function $\phi$.

-   There is then another linear model with $M+1$ parameters for the predicted target.

-   This allows a **lot** of flexibility.
---

.left-code[
## Sigmoid Activation Function

-   Common choice of $\phi$ is the sigmoid function
    $\phi(v) = \tfrac{1}{1 + e^{-v}}$.

-   Note that large input $v > 4$ returns almost one; small input $v < -4$
    returns almost zero.
]

.right-plot[
```{r, echo=FALSE}
ggplot() +
  geom_function(fun = ~1/(1+exp(-.x)), xlim=c(-5, 5)) +
  labs(x = 'v',
       y = expression(phi(v)))
```
]

---

## Fitting A Neural Network

-   A neural network is defined by large number of parameters:
    $\alpha_{01}, \alpha_{11}, \ldots, \alpha_{pM}, \beta_0, \ldots, \beta_{M}$.

-   These are often referred to as *weights* for the neural net.

-   The $M(p+1)+M+1$ weights must be estimated by fitting the model to training data.

-   We aim to select the weights that minimize the residual sum of
    squares for predictions from the neural network.

-   Not straightforward because of the complexity of the model.

    -   Consider the sheer number of parameters that need to be
        estimated, complicated by the non-linear transformation.

-   In practice neural networks are fitted using the *back
    propagation algorithm*.

-   This involves a complicated search over possible weights, and can be
    dependent on 'initial values' (i.e. search start point).

---

## Basic idea of back propagation:

1.  We generate random initial starting weights.

2.  We 'feed-forward' the training data through the layers to give a prediction $\hat{y}$.

3.  We compute the residual sum of squares RSS. This is our *loss function* that we want to minimise.

4.  RSS is a complicated function of the weights, but it's construction means we can compute the partial derivatives with respect to each weight quite easily (i.e. the direction and amount each weight needs to move to decrease RSS).

5.  We use *batch gradient descent* to move all the weights $w$ a step in the right direction. The step size is known as the *decay* - it's basically to ensure we don't overshoot the minimum RSS.

6.  We take the step, update the weights, then loop back to step 2.

---

## Implementing Neural Networks in R

-   R function for fitting a neural network is `nnet()` from the `nnet` package.

-   The syntax to fit a neural network is of the familiar form

        nnet(y ~ x1 + x2 + x3, data=mydata, size=2, linout=TRUE)

-   `size` argument specifies the number of nodes in the hidden layer.
    It has no default value.

-   The argument `linout` is logical, and indicates whether the
    relationship between the prediction (output $\hat y$) and the
    derived features is linear or not.

    -   The default setting for `linout` is `FALSE`, when `nnet` produces
        output constrained to the interval $[0,1]$ by applying an additional
        sigmoid activation function.

    -   For prediction problems we must set `linout=TRUE`.

---

## Fitting Neural Networks in R

-   `nnet` has optional arguments controlling model fitting.

-   `decay` controls the weight updating rate (learning rate) in back propagation algorithm.

    -   Setting `decay` to a value such as $0.01$ or $0.001$ can be useful to speed up or slow down learning.

-   The model fitting algorithm used by `nnet` needs initial values for
    the weights.

-   By default these randomly chosen on the interval $[-0.5,0.5]$.

-   Final values of the weights can be quite dependent upon the choice
    of initial values,

    -   Means fitted nets can vary even when rerunning same code.

    -   Can suppress this variation by setting the random number seed
        with the `set.seed()` command.

-   `nnet` uses a small number of iterations (100) by default. Increasing this can be required if the decay/learning rate is low. We set this by specifying `maxit`.

---

## Neural Networks for the Wage Data

-   We will train a neural network on wage data.

-   For the number of hidden nodes to use initially, we need the number of
    effective predictors.

-   There are 5 numerical predictors:

    -   `EDU`, `SOUTH`, `EXP`, `UNION`, and `AGE`;

-   There are 5 factors:

    -   `SEX` (with 2 levels), `RACE` (3 levels), `OCCUP` (6 levels),
        `SECTOR` (3 levels) and `MARRIAGE` (2 levels).

-   Gives effective number predictors $p = 5 + 1 + 2 + 5 + 2 + 1 = 16$.

-   Suggests we try $p/2 = 8$ nodes in the hidden layer.

---

.left-code-wide[
## Neural Networks for the Wage Data

```{r wagenn1, eval=FALSE}
library(nnet)
set.seed(1069)
wage.nn.1 <- nnet(WAGE ~ .,size=8,
                  data=wage.train,
                  linout=TRUE)
```

This first model has 8 hidden nodes. How many weights?

-   $p+1 = 17$ weights per hidden node (number of $\alpha$s);

-   $8+1=9$ weights for output node (number of $\beta$s);

-   So $17\times 8 + 9 = 145$ weights to be determined.

The neural network has stopped learning
after 100 iterations - it has not converged.
]

.right-plot-narrow[
```{r, ref.label='wagenn1', echo=FALSE}
```
]

---

.left-code-wide[
## Neural Networks for the Wage Data

```{r wagenn2, eval=FALSE}
set.seed(1069)
wage.nn.1 <- nnet(WAGE ~ .,size=8,
                  data=wage.train,
                  linout=TRUE,
                  maxit=250) #<<
```

Giving more iterations enables it to converge.
]

.right-plot-narrow[
```{r, ref.label='wagenn2', echo=FALSE}
```
]

---

.left-code-wide[
## Neural Networks for the Wage Data

```{r wagenn3, eval=FALSE}
set.seed(1069)
wage.nn.2 <- nnet(WAGE ~ .,size=8,
                  data=wage.train,
                  linout=TRUE,
                  maxit=1000,
                  decay=0.01) #<<
```

Using a different `decay` value can
sometimes allow it to converge to a different
place.

Because we have so many parameters, there
are often multiple minima available.

Running with different seeds and different
decay rates can change which minima we settle
in (which may result in a lower RSS, so a
better fit).
]

.scroll-box-right.small-font.right-plot-narrow[
```{r, ref.label='wagenn3', echo=FALSE}
```
]

---

.left-code-wide[
## Neural Networks for the Wage Data

```{r wagenn4, eval=FALSE}
set.seed(1069)
wage.nn.3 <- nnet(WAGE ~ .,
                  size=5, #<<
                  data=wage.train,
                  linout=TRUE,
                  maxit=250)
```

The `size` parameter can be worth tuning,
as it heavily influences the flexibility
of the resulting model.

This model has 91 weights.
]

.scroll-box-right.small-font.right-plot-narrow[
```{r, ref.label='wagenn4', echo=FALSE}
```
]

---

.left-code-wide[
## Neural Networks for the Wage Data

```{r wagenn5, eval=FALSE}
set.seed(1069)
wage.nn.4 <- nnet(WAGE ~ .,
                  size=5, #<<
                  data=wage.train,
                  linout=TRUE,
                  decay=0.01, #<<
                  maxit=1000)
```

We often need to tune both, and there's
not necessarily a systematic way of doing
this well (other than brute force) due
to the complexity of the model.
]

.scroll-box-right.small-font.right-plot-narrow[
```{r, ref.label='wagenn5', echo=FALSE}
```
]

---

## Neural Networks for the Wage Data
.left-code-wide[
```{r wagenetsumm, eval=FALSE}
wage.test |>
  bind_cols(
    nn.1 = predict(wage.nn.1, newdata=wage.test),
    nn.2 = predict(wage.nn.2, newdata=wage.test),
    nn.3 = predict(wage.nn.3, newdata=wage.test),
    nn.4 = predict(wage.nn.4, newdata=wage.test),
  ) |>
  summarise(
    across(starts_with('nn.'), ~mean((. - WAGE)^2)))
```
```{r wagenetsumm2, eval=FALSE}
wage.train |>
  bind_cols(
    nn.1 = predict(wage.nn.1, newdata=wage.train),
    nn.2 = predict(wage.nn.2, newdata=wage.train),
    nn.3 = predict(wage.nn.3, newdata=wage.train),
    nn.4 = predict(wage.nn.4, newdata=wage.train),
  ) |>
  summarise(
    across(starts_with('nn.'), ~mean((. - WAGE)^2)))
```
]

.right-plot-narrow[
Test data:
```{r, ref.label="wagenetsumm", echo=FALSE}
```

Training data:
```{r, ref.label="wagenetsumm2", echo=FALSE}
```

**Beware of overfitting!**

`wage.nn.2` gave a much lower RSS in training, but
gave very poor predictions on the test data!
]

---

## Summary of Neural Network Prediction

-   Neural networks are extremely flexible for prediction.

-   Complexity depends on number of hidden nodes.

    -   Bias-variance trade-off applies in theory: increasing number of
        hidden nodes will reduce bias but increase variance.

    -   In practice the picture is confused by the difficulties in model
        fitting.

-   Training neural nets is a complex problem.

    -   Fitting methods are less stable than for linear models and
        regression trees or forests.

    -   Numerical issues can arise when predictors are on (very) different
        scales, particularly if large in magnitude.

    -   Pre-scaling predictors can be advisable.

---

class: middle,inverse

# Tidy models

---

## Tidy models

We've seen how to do prediction using linear models, regression trees, forests,
and neural networks.

In each case both the model fitting and obtaining predictions was consistent:

- We use a formula syntax to specify the model and provide the training data.
- We use the `predict` command on the resulting model object, providing testing data.

This works well as long as all our models follow this same syntax. Unfortunately,
many models require different syntaxes for one or both of these steps.

We also likely want to process the data beforehand.

- Impute missing values
- Convert predictors to a common scale.

And we'd potentially be better to assess model predictions using metrics
other than MSE.

The `tidymodels` set of packages deals with these tasks.

---

## Tidymodels

We will look at just part of the `tidymodels` framework, focusing on four packages:

- `rsample` for resampling.
- `recipes` for data processing.
- `parsnip` for model fitting.
- `yardstick` for measuring model performance.

The [tidymodels website](https://tidymodels.org) has a number of excellent tutorials and an overview of some of the additional packages available (e.g. `tune`, `workflows`).

We'll demonstrate these via an example.

---

## Example: COVID-19 vaccinations

In October 2021, the Ministry of Health released the first set of COVID-19 vaccination data at a fine spatial scale, statistical area units level 2, or `SA2`. In urban areas, each SA2 covers around 2,000 to 4,000 residents. In highly rural areas the areas are larger but can contain fewer (less than 1,000) residents.

```{r, message=FALSE}
vacc <- read_csv("https://www.massey.ac.nz/~jcmarsha/data/covid19/covid-vacc.csv")
vacc
```

---

## Example: COVID-19 vaccinations

Unfortunately, the `VaccRate` data has become corrupted, and `r vacc |> filter(is.na(VaccRate)) |> nrow()` SA2s are missing `VaccRate`:

```{r, echo=FALSE, skimr_include_summary = FALSE}
skim(vacc |> select(where(is.numeric)), -sa2)
```

Out goal is to predict `VaccRate` in the regions where it is unknown. We also have some map data:

```{r, message=FALSE, warning=FALSE}
library(sf)
sa2_map <- read_sf("https://www.massey.ac.nz/~jcmarsha/data/covid19/sa2_boundary.sqlite")
```

---

.left-code[
## Example: COVID-19 vacc

```{r covidreg, eval=FALSE}
sa2_map |>
  left_join(vacc) |>
  ggplot() +
  geom_sf(aes(fill=VaccRate),
          size=0.1) +
  scale_fill_viridis_c(
    option="A",
    begin=0.5,
    na.value = 'grey30'
  )
```

We see that vaccination rates in an area
are similar to neighbouring areas.

But the regions that are missing are mostly
spatially contiguous.

This will make things harder, as we can't
rely on spatial auto-correlation.
]

.right-plot[
```{r, ref.label="covidreg", echo=FALSE, message=FALSE}
```
]

---

## Example: COVID-19 vaccination

We'll start by dividing into training and test sets:

```{r}
vacc.train <- vacc |> filter(!is.na(VaccRate))
vacc.test  <- vacc |> filter(is.na(VaccRate)) |> select(-VaccRate)
```

Out goal now will be to build some models with `vacc.train` and apply these to do prediction on `vacc.test`.

As we don't know the target in `vacc.test`, we will set it aside and concentrate our modelling efforts on `vacc.train`.

---

.left-code[
## Example: COVID-19 vacc

We start by plotting the relationships between the covariates and vaccination rates.

First the numeric measures:

```{r vaccrel, eval=FALSE}
vacc.train |>
  pivot_longer(c(Population,
                 SocialDeprivation,
                 DistanceToVacc,
                 PropUnder30)) |>
  ggplot() +
  geom_point(aes(x=value,
                 y=VaccRate),
             alpha=0.4) +
  facet_wrap(vars(name),
             scales='free_x') +
  labs(x=NULL)
```
]

.right-plot[
```{r, ref.label="vaccrel", echo=FALSE}
```
]

---

.left-code[
## Example: COVID-19 vacc

And now the factors:

```{r vaccrel2, eval=FALSE}
vacc.train |>
  pivot_longer(
    c(DHB, TertiaryStudents)) |>
  ggplot() +
  geom_boxplot(aes(x=VaccRate,
                   y=value)) +
  facet_wrap(vars(name),
             scales='free_y') +
  labs(y=NULL)
```

It seems all of these variables
are important for vaccination rates.
]

.right-plot[
```{r, ref.label="vaccrel2", echo=FALSE}
```
]
---

## Example: COVID-19 vacc

We'll now start modelling. Our process will be:

1. Splitting our training data into training and validation sets.

    - We need an independent set from that used to train models in order to assess model performance.

2. Processing the data so that the numeric measures are on similar scales.

    - This is particularly important for neural networks to avoid computational difficulties.
    - But also we might like to do a transformation. e.g. the `DistanceToVacc` variable might warrant
    a log transformation.

3. Model training using the processed training data.

4. Prediction on the processed validation data.

5. Select the best model.

6. Re-fit the best model on all the training data and use it to predict the unknown vaccination rates on the test data.

---

## Step 1: Splitting the data

We'll utilise the `rsample` package in `tidymodels` for splitting data. The `initial_split()` function is useful for a single split:

```{r}
library(rsample)
split <- initial_split(vacc.train, prop=0.75)
split
```

The `split` object contains information on how the data should be split: which rows should be assigned to the dataset for analysis, and which should be assigned to the dataset for assessment or validation.

We can extract the datasets using `training()` and `testing()` functions:

```{r}
vacc.analysis <- training(split)
vacc.validation <- testing(split)
```

---

## Step 2: Prepare the data

We'll utilise the `recipes` package in `tidymodels` to codify the operations we wish to perform on the data. This ensures our operations are consistent when applied to multiple datasets.

- If we're scaling data, then the amount we scale in both datasets must be exactly the same.

We start by creating a `recipe` using a model formula and some template data

- The dataset passed in is just a template to derive the variable names and roles. This isn't model fitting.

```{r, message=FALSE}
library(recipes)
vacc.base <- recipe(VaccRate ~ ., data=vacc.analysis)
vacc.base
```

---

## Step 2: Prepare the data

.left-code[

```{r vaccrec, eval=FALSE}
vacc.rec <- vacc.base |>
  update_role(sa2,
              new_role="id") |>
  step_log(DistanceToVacc) |>
  step_normalize(
    all_numeric_predictors())

vacc.rec
```

The `sa2` variable (region) is a row identifier
so shouldn't be used for prediction.

The `DistanceToVacc` variable will be log transformed.

All numeric variables will be normalised to a common scale
with mean 0 and variance 1.
]

.right-plot[
```{r, ref.label="vaccrec", echo=FALSE}
```
]
---

## Step 2: Prepare the data

Now that we have our recipe, we can prepare it using the analysis data set. This will train the various data transformations:

```{r}
vacc.prep <- vacc.rec |> prep(vacc.analysis)
vacc.prep
```

---

## Step 2: Preparing the data

Finally, we can bake our prepared recipe to produce the final analysis set:

```{r, eval=FALSE}
vacc.analysis.baked <- vacc.prep |> bake(vacc.analysis)
skim(vacc.analysis.baked)
```
```{r, echo=FALSE, skimr_include_summary = FALSE}
vacc.analysis.baked <- vacc.prep |> bake(vacc.analysis)
skim(vacc.analysis.baked |> select(where(is.numeric), -sa2))
```

---

## Step 2: Preparing the data

The final validation set is baked in the same way:

```{r, eval=FALSE}
vacc.validation.baked <- vacc.prep |> bake(vacc.validation)
skim(vacc.validation.baked)
```
```{r, echo=FALSE, skimr_include_summary = FALSE}
vacc.validation.baked <- vacc.prep |> bake(vacc.validation)
skim(vacc.validation.baked |> select(where(is.numeric), -sa2))
```
Note the mean and sd are not 0 and 1, because the transformation was trained
using the `vacc.analysis` set and we've used the same scaling coefficients here.

---

## Step 3: Modelling

Now we have our baked data, we can start training models. We'll use the `parsnip` package for this.

The `parsnip` package provides a consistent interface for different model types across different fitting engines.

- Model types generally define the structure of the model.
- Different engines can and do produce different estimators for a particular model.

For a linear model, we use `linear_reg()` to setup the regression model. We can fit the linear regression using:
 - least squares with `lm`.
 - penalised least squares (LASSO/Ridge) with `glmnet`.
 - a Bayesian framework with `stan`.
 
`parsnip` unifies the interface across all the different model types and engines.
- The same parameter names are used across engines.
- Everything has a `fit()` and `predict()` function that act in predictable ways.

---

## Step 3: Modelling

Let's fit a linear regression, a random forest and a neural network.

For the neural network we'll use 12 hidden units<sup>1</sup> and 10,000 epochs (iterations).

```{r, message=FALSE}
library(parsnip)
spec.lm <- linear_reg(mode = "regression", engine = "lm")
spec.rf <- rand_forest(mode = "regression", engine = "randomForest")
spec.nn <- mlp(mode = "regression", engine="nnet", hidden_units = 12, epochs = 10000)
```

We then fit the models with `fit()`. Notice how this is consistent for all models:

```{r}
set.seed(9)
fit.lm <- spec.lm |> fit(VaccRate ~ . - sa2, data=vacc.analysis.baked)
fit.rf <- spec.rf |> fit(VaccRate ~ . - sa2, data=vacc.analysis.baked)
fit.nn <- spec.nn |> fit(VaccRate ~ . - sa2, data=vacc.analysis.baked)
```

.footnote[
[1] We have $p = 4 + (20-1) + (2-1) = 24$ effective predictors.
]
---

.left-code[## Step 3: Modelling

If we like, we can extract the underlying model
object to interrogate it:

```{r parsniplm, eval=FALSE}
fit.lm |>
  extract_fit_engine() |>
  summary()
```

```{r, echo=FALSE}
lm.tidy <- fit.lm |> glance()
```

- Larger population centers are more vaccinated.
- More young people lowers vaccination.
- Increased social deprivation lowers vaccination.
- Increased distance to vaccination lowers vaccination.
- Places with tertiary students are more vaccinated.

The overall model fit isn't too bad, explaining `r round(100*lm.tidy$r.squared,1)`%
of the variation.
]

.scroll-box-right.small-font.right-plot[
```{r, ref.label="parsniplm", echo=FALSE}
```
]

---

## Step 4: Prediction

.left-code-wide[
We then do prediction from our fitted models using our baked validation set:

```{r parsnippred, eval=FALSE}
pred.lm <- fit.lm |>
  predict(new_data=vacc.validation.baked)
pred.rf <- fit.rf |>
  predict(new_data=vacc.validation.baked)
pred.nn <- fit.nn |>
  predict(new_data=vacc.validation.baked)

vacc.validation |>
  select(VaccRate) |>
  bind_cols(pred.lm |> rename(lm = .pred),
            pred.rf |> rename(rf = .pred),
            pred.nn |> rename(nn = .pred))
```

`predict` returns a `tibble` with a `.pred` column,
so needs to be renamed if you want separate columns.

There is correlation between `VaccRate` and predictions.
]

.right-plot-narrow[
```{r, ref.label="parsnippred", echo=FALSE}
```
]

---

.left-code-wide[
## Step 4: Prediction

It's tidier to bind the predictions into one column:

```{r plotpred, eval=FALSE}
pred.lm <- vacc.validation |>
  bind_cols(fit.lm |> predict(vacc.validation.baked))
pred.rf <- vacc.validation |>
  bind_cols(fit.rf |> predict(vacc.validation.baked))
pred.nn <- vacc.validation |>
  bind_cols(fit.nn |> predict(vacc.validation.baked))

pred <- bind_rows(
  lst(pred.lm, pred.rf, pred.nn),
  .id="model")

pred |> ggplot() +
  geom_abline(slope=1, intercept=0) +
  geom_point(aes(x=.pred, y=VaccRate), alpha=0.3) +
  facet_wrap(vars(model), ncol=1)
```

The predictions generally look reasonable.

The neural net and linear regression have predicted vaccination rates over 1.
]

.right-plot-narrow[
```{r, ref.label="plotpred", echo=FALSE, fig.dim=c(3.2,4.5)}
```
]

---

## Step 5: Model selection

To evaluate the best model we can use the `yardstick` package.

The `metrics` function gives us the root mean squared error, mean absolute error and $R^2$ statistic
for prediction:

```{r, message=FALSE}
library(yardstick)
pred |>
  group_by(model) |>
  metrics(truth=VaccRate, .pred) |>
  pivot_wider(names_from=.metric, values_from=.estimate)
```

For all metrics, the random forest is best.

---

## Step 6: Predicting the unknown

Now that we know which model is best<sup>1</sup> we can now re-train it using all the training data (rather than just our analysis subset) and use this to do prediction for the unknown vaccination rates:

```{r testfit, eval=FALSE}
trained_fit <- spec.rf |> fit(VaccRate ~ . -sa2, bake(vacc.prep, vacc.train))

vacc.test.pred <- vacc.test |>
                    bind_cols(
                      predict(trained_fit, bake(vacc.prep, vacc.test))
                    ) |>
                    rename(VaccRate = .pred)
vacc.test.pred |> slice_head(n=5)
```

```{r, echo=FALSE}
old_width <- options(width=100)
```

.small-font[
```{r, ref.label="testfit", echo=FALSE}
```
]

```{r, echo=FALSE}
options(width=old_width[["width"]])
```

.footnote[
[1] We haven't tuned these models, so they may not be best!
]

---

.left-code[
## Step 6: Predicting the unknown

```{r predmap, eval=FALSE}
vacc_all <- bind_rows(
  list(train=vacc.train,
       test=vacc.test.pred),
       .id="set")

sa2_map |>
  left_join(vacc_all) |>
  ggplot() +
  geom_sf(aes(fill=VaccRate),
          size=0.1) +
  scale_fill_viridis_c(
    begin=0.5,
    option="A"
  )
```

We now have predictions for the
vaccination rates in all regions.
]

.right-plot[
```{r, ref.label="predmap", message=FALSE, echo=FALSE}
```
]
