---
title: 'Lecture 5'
subtitle: 'Trees and Forests'
output:
  xaringan::moon_reader:
    css: [default, default-fonts, "custom.css"]
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
    nature:
      navigation:
        scroll: false
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(4.8, 4.5), fig.retina=2, out.width="100%", comment="#>")
theme_set(theme_minimal())

wage.train <- read_csv("../data/wage-train.csv")
wage.test  <- read_csv("../data/wage-test.csv")
```

## Regression Trees

-   Linear regression models make strong assumptions about the
    functional form of the relationship between target and predictor
    variables.

-   Such models work very well when these assumptions are valid, and can
    fail spectacularly when they are not.

-   In this lecture we introduce regression trees, which provide a more
    flexible approach to prediction.

-   Regression trees are sometimes referred to by alternative names:

    -   CART (classification and regression trees);

    -   Recursive partitioning methods.

-   Random forests are then collections of trees.

---

## Overview of Regression Trees

-   The basic idea is to split the data into groups using the predictors, and
    then estimate the response within each group by a fixed value.

-   Most commonly, the groups are formed by a sequence of **binary splits**.

-   The resulting partition of the data can be described by a **binary tree**.

-   Perhaps easiest to understand through an example.

---

.left-code[## Regression Tree for a Toy Dataset

-   Artificial dataset of 400 observations with a single predictor $x$
    and a target $y$.

-   Relationship between $x$ and $y$ is quite complicated.

-   It would be difficult to model using linear regression.
]

.right-plot[
```{r toytree1, echo=FALSE}
set.seed(2012)
x <- runif(400,0,10)
x <- sort(x)
y <- 1+ sin(1.5*x/(1+x^2/100)) + rnorm(400,sd=0.25)
toytree <- data.frame(x=x, y=y)
ggplot(toytree) +
  geom_point(mapping = aes(x=x, y=y))
```
]

---

.left-code-wide[## Example continued

```{r, echo=FALSE}
library(rpart)
toy.rp <- rpart(y ~ x, cp=0.1, data=toytree)
toy_splits <- round(toy.rp$splits[,"index"],3)
```

-   A tree is formed by recursive binary partitioning of data.

-   All data starts at the 'root' node at the top.

-   The first split branching the tree is at its root.

    -   Data with $x < `r toy_splits[1]`$ branch left.

    -   Data with $x \ge `r toy_splits[1]`$ branch right.

-   A second split further divides the $x < `r toy_splits[1]`$ group to form two
    sub-groups.

    -   Data with $x \ge `r toy_splits[2]`$ branch left.

    -   Data with $x < `r toy_splits[2]`$ branch right.
]

.right-plot-narrow[
```{r, echo=FALSE, fig.dim=c(3.2,4.5)}
plot(toy.rp,compress=TRUE,margin=0.15)
text(toy.rp,cex=1)
```
]

---

.left-code[## Example continued

-   In principle we could partition the data further.

-   For simplicity we will stop with the current set of **leaves**.

-   At each leaf the target is estimated by the mean value of the
    y-variable for all data at that leaf.

-   This means that the predictions are constant across the range of
    x-values defining each leaf.
]

.right-plot[
```{r, echo=FALSE}
rect1 <- data.frame(x=c(-1,-1,toy_splits[2],toy_splits[2]), y=c(-0.7,2.9,2.9,-0.7), col=grey(0.95))
rect2 <- data.frame(x=c(toy_splits[2],toy_splits[2],toy_splits[1],toy_splits[1]), y=c(-0.7,2.9,2.9,-0.7), col=grey(0.85))
rect3 <- data.frame(x=c(toy_splits[1],toy_splits[1],11,11),y=c(-0.7,2.9,2.9,-0.7), col=grey(0.95))
rects <- bind_rows(lst(rect1, rect2, rect3), .id="rect")
segs <- bind_rows(lst(seg1 = data.frame(x = c(toy_splits[1],11), y = 1.794),
                      seg2 = data.frame(x = c(toy_splits[2], toy_splits[1]), y = 0.5704),
                      seg3 = data.frame(x = c(-1, toy_splits[2]), y = 1.645)), .id="seg")

ggplot(toytree) +
  geom_polygon(data = rects, mapping = aes(x=x, y=y, fill=col)) +
  scale_fill_identity() +
  geom_point(mapping = aes(x=x, y=y), col="#DF536B") +
  geom_line(data=segs, mapping = aes(x=x, y=y, group=seg), size=1) +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0))
```
]

---

## How to Grow a Tree

-   At each stage in tree growth, we want to select the best split.

    -   Which node to split at?

    -   Which variable to split with?

    -   What value of that variable to split at?

-   The best split is usually regarded as the one which results in smallest
    possible *residual sum of squares* on the training data:
    $$\mathsf{RSS}_{train} = \sum_{i=1}^n \left ( y_i - \hat y_i \right )^2$$

-   Optimal (to minimize RSS) to define $\hat y_i$ to be the mean
    value at leaf containing record $i$. All predictions at leaf $\ell$
    are assigned the value $\bar y_{\ell}$.

-   The residual sum of squares for a tree can then be written as
    $$\mathsf{RSS} = \sum_{\ell} \sum_{i \in C_{\ell}} \left ( y_{i} - \bar y_{\ell} \right )^2$$
    where the set $C_{\ell}$ indexes the observation at leaf $\ell$.

---

## A Simple Choice of Splits
.left-code[
-   Suppose we have just 3 observations:

| $x$ | $y$ |
| --: | --: |
| 0.1 |  2  |
| 0.3 |  3  |
| 0.5 |  7  |

-   It is clear that we need consider only two splits:

    (i) split the data according to whether $x < 0.2$ or $x \ge 0.2$; or

    (ii) split the data according to whether $x < 0.4$ or $x \ge 0.4$.
]
--
.right-plot[
For tree (i):

-   Mean for left-hand leaf is $\bar{y}_1 = 2$

-   Mean for right-hand leaf is $\bar{y}_2 = (3+7)/2 = 5$.

$$\mathsf{RSS} = \left [ (2-2)^2 \right ] + \left [ (3-5)^2 + (7-5)^2 \right ] = 8.$$
]

--
.right-plot[
For tree (ii):

-   Mean for left-hand leaf is $\bar{y}_1 = (2+3)/2 = 2.5$

-   Mean for right-hand leaf is $\bar{y}_2 = 7$.

$$\mathsf{RSS} = \left [ (2-2.5)^2 + (3-2.5)^2 \right ] + \left [ (7-7)^2 \right ] = 0.5.$$
]

--
.right-plot[
**We conclude that tree (ii) is preferable.**
]

---

## Categorical Predictor Variables

-   So far we have dealt only with splits based on numerical predictors.

-   Factors are split by dividing the factor levels into two sets.

-   Observations are then sent down the left or right branch according to this
    division.

-   The separation of the levels is again done so as to minimize the
    residual sum of squares.

-   Note that for factors with many levels, there are a (very) large number
    of possible splits to consider.

-   If there are $k$ levels, then there are $2^{k-1}-1$ possible splits to consider.

---

.left-code[
## Regression Tree for the Wage Data

```{r wagerp, eval=FALSE}
library(rpart)
wage.rp <- rpart(WAGE ~ . ,
                 data = wage.train)
summary(wage.rp)
```

The `rpart()` command fits a regression tree,
using a similar syntax to `lm()`.

Note copious output from `summary()`.

We see the first split is on `OCCUP` where
the first, third, fifth and sixth levels
go down the left branch, while the second
and fourth levels go to the right.

This split was chosen as it improved the RSS by 17.2%.
The next best option is to split on `EDU < 13.5`.
]

.scroll-box-right.small-font.right-plot[
```{r, ref.label="wagerp", echo=FALSE}
```
]

---

.left-code[
## Regression Tree for the Wage Data

```{r wagerp2, eval=FALSE}
plot(wage.rp,
     compress = TRUE,
     margin = 0.1)
text(wage.rp)
```

The `plot` command visualises the tree,
whilst the `text` command adorns it with
labels.

The `compress=TRUE` tends to make the tree
more visually appealing, while `margin=0.1`
adds a bit of whitespace.

Note that factor levels are reassigned the values
a, b, c etc.
]

.right-plot[
```{r, echo=FALSE}
par(mar=c(0,0,0,0))
plot(wage.rp,
     compress = TRUE,
     margin = 0.1)
text(wage.rp)
```
]

---

.left-code[
## Missing Data and Surrogate Splits

-   Regression trees can handle missing data using *surrogate splits*.

-   A surrogate split is an alternative that approximates the best
    primary split.

-   Surrogate split on the first node is `EDU` $< 14.5$.

-   Any record with a missing value on `OCCUP` will split according
    to this surrogate.
]

.scroll-box-right.small-font.right-plot[
```{r, ref.label="wagerp", echo=FALSE}
```
]
---

## Prediction Using Regression Trees

Prediction works by running each test case down the tree, using surrogate splits where necessary.

```{r}
wage.pred.rp <- 
  wage.test %>%
  bind_cols(
    .pred = predict(wage.rp, newdata=wage.test)
  )
wage.pred.rp %>%
  summarise(MSE =  mean((.pred - WAGE)^2))
```

-   MSE of prediction for regression tree is $24.4$.

-   MSE of prediction with the linear model was $21.9$.

-   This suggests that the relationship between target and predictors is
    reasonably linear.

---

## Pruning Trees

-   In principle we can continue to split the nodes of a tree until there is
    one leaf corresponding to each unique observed set of predictor
    variables.

    -   If predictor sets are different for all records, then this
        implies a single data point at each leaf.

-   This would lead to a highly unstable regression tree.

-   Consider the bias-variance trade off:

    -   One observation per leaf implies lots of flexibility in the model
        (so low bias) but high variability.

    -   Many observations per leaf reduce flexibility (introduce bias)
        but reduce variability.

---

.left-code[
## Return of the Toy Dataset

-   Optimal prediction will aim for 'predictable' trend.

-   We have no hope of predicting the 'noise' based on the single available predictor.
]

.right-plot[
```{r, echo=FALSE}
ggplot(toytree) +
  geom_point(mapping = aes(x=x, y=y))
```
]
---

.left-code[## Example continued

-   Recall that regression trees imply a 'step function' estimation of the trend.

-   The more complex the tree, the more complex the step function!
]

.right-plot[
```{r, echo=FALSE}
toy.rp.1 <- rpart(y ~ x, cp=0.1, data=toytree)
toy.rp.2 <- rpart(y ~ x, cp=0.01, data=toytree)
toy.rp.3 <- rpart(y ~ x, cp=0.000001, data=toytree, minsplit=3)
par(mar=c(3,1,1,1))
par(mfrow=c(3,2))
plot(toy.rp.1,compress=T,margin=0.1)
plot(x,y,col=2,pch=19)
lines(x,predict(toy.rp.1),lwd=2)
plot(toy.rp.2,compress=T,margin=0.1)
plot(x,y,col=2,pch=19)
lines(x,predict(toy.rp.2),lwd=2)
plot(toy.rp.3,compress=T,margin=0.1)
plot(x,y,col=2,pch=19)
lines(x,predict(toy.rp.3),lwd=2)
```
]

---

## Model Complexity

-   We can think about the bias-variance trade off in terms of model
    complexity.

-   Low complexity - high bias - low variability.

-   High complexity - low bias - high variability.

-   This motivates the introduction of the **complexity parameter** `cp`.

-   `cp` specifies the minimum improvement in model fit that warrants
    inclusion of a new split in the tree.

-   Implies tree growth ceases when no split produces an improvement of
    at least `cp`.

-   `cp` is the relative improvement in residual sum of squares.

-   Specified by `cp` argument of `rpart()`, default `cp=0.01`.

---

.left-code[## Regression Trees of different complexities

```{r}
wage.rp.2 <- rpart(WAGE ~ .,
                   data=wage.train,
                   cp=0.1)

wage.rp.3 <- rpart(WAGE ~ .,
                   data=wage.train,
                   cp=0.001)
```

There is a clear difference in complexity here!
]

.right-plot[
```{r, echo=FALSE}
par(mfrow=c(2,1), mar=c(0,0,1,0))
plot(wage.rp.2,compress=T,margin=0.1)
text(wage.rp.2,cex=0.7)
title("cp=0.1")
plot(wage.rp.3,compress=T,margin=0.1)
text(wage.rp.3,cex=0.7)
title("cp=0.001")
```
]

---

## MSE of Prediction for Models

```{r}
wage.rp.pred <- wage.test %>%
  bind_cols(
    .pred1 = predict(wage.rp, newdata=wage.test),
    .pred2 = predict(wage.rp.2, newdata=wage.test),
    .pred3 = predict(wage.rp.3, newdata=wage.test)
  )
wage.rp.pred %>%
  summarise(MSE.1 = mean((.pred1 - WAGE)^2),
            MSE.2 = mean((.pred2 - WAGE)^2),
            MSE.3 = mean((.pred3 - WAGE)^2))
```

Prediction using `cp=0.001` is better than the default value of `cp=0.01` in this case.

---

## Validation

-   The default value of $cp=0.01$ is only a rule-of-thumb.

-   We can often get better predictions by adjusting this parameter.

-   Bu there is no use tuning using the training data - we will always select the most complex
    tree, overfitting.

-   One approach is to divide the available data into training and
    **validation** datasets.

    -   Usually the training set is larger - e.g. training to validation size
        ratio of 3:1, or even 9:1.

-   Then use the training data for growing the tree.

-   Predictive accuracy is then assessed by predicting on the validation dataset.

-   Pick the value of `cp` that minimizes prediction error.

---

## Cross-Validation

-   Validation results will depend on the particular decomposition into training
    and validation subsets.

-   Cross-validation addresses this issue.

-   The idea is that we split the data into equally size blocks
    (subgroups).

-   Each block in turn is set aside as the validation data, with the
    remaining blocks combining to form the training set.

-   The prediction error is then averaged over the results for each validation set.

-   Common to use *10-fold* cross-validation where the data are split into 10
    blocks (so 90% to 10% training to validation ratio at each stage).

    -   Extreme version is 'leave-one-out' cross-validation.

-   Can be run for different values of `cp`.

---

.left-code[## Cross-Validation for the Wage Data Trees

A table of model characteristics for different values of `cp` can be
obtained using the command `printcp`.

```{r wagecp, eval=FALSE}
printcp(wage.rp.3)
```

The `xerror` column contains cross-validation estimates of the
(relative) prediction error.

`xstd` is the standard error (i.e. an estimate of the uncertainty)
for these cross-validation estimates.
]

.small-font.right-plot[
```{r, ref.label='wagecp', echo=FALSE}
```
]

--
.left-code[**No real evidence to prefer any tree past 1 split!**
]

---

class: middle,inverse

# Forests

---

## Tree instability

- Decision tree models are known to be **unstable**.

- When many variables are available, there are many potential competing splits that serve to improve the model fit by around the same amount.

- The choice of which split to use can then result in very different subtrees.

- We'll illustrate this by producing a tree and then resample the training data, re-selecting
the same number of observations with replacement (*bootstrapping*) and produce a second tree.

- Ideally the trees should not differ much as the data come from the same underlying population.

---

.left-code[
## Tree instability

```{r wagebs, eval=FALSE}
set.seed(5)
wage.train.bs <- slice_sample(
                   wage.train,
                   n=400,
                   replace=TRUE)

bind_rows(lst(wage.train,
              wage.train.bs),
          .id="data") %>%
  select(data, WAGE,
           AGE, EDU, EXP) %>%
  pivot_longer(-c(data, WAGE)) %>%
  ggplot() +
  geom_point(aes(x=value, y=WAGE),
             alpha=0.5) +
  facet_grid(rows=vars(data),
             cols=vars(name),
             scales='free_x') +
  labs(x=NULL)
```

The distributions of the two datasets are similar.
]

.right-plot[
```{r, ref.label="wagebs", echo=FALSE}
```
]

---

.left-code[
## Tree instability

```{r wagebstree, eval=FALSE}
m1 <- rpart(WAGE ~ .,
            data=wage.train)
m2 <- rpart(WAGE ~ .,
            data=wage.train.bs)
plot(m1, margin=0.1)
text(m1)
plot(m2, margin=0.1)
text(m2)
```

The trees are very different.

Even the variable at the first split differs.
]

.right-plot[
```{r, echo=FALSE}
m1 <- rpart(WAGE ~ ., data=wage.train)
m2 <- rpart(WAGE ~ ., data=wage.train.bs)
par(mfrow=c(2,1), mar=c(0,0,2,0))
plot(m1, margin=0.1)
text(m1)
title("wage.train")
plot(m2, margin=0.1)
text(m2)
title("wage.train.bs")
```
]

---

## Random forests

A random forest is a collection of decision trees (e.g. regression trees) generated by applying two separate randomisation processes:

1. The data (rows) are randomised through a bootstrap resample.
2. A random selection of predictors (columns) are considered for each split, rather than considering all variables.

Once done, a tree is fit to the data. The process is then repeated, and many trees are generated. Predictions are then averaged across all the trees in the forest.

The forest **benefits** from the instability of the trees.

 - Each bootstrap resample is likely to generate a different tree as tree building is brittle.
 - Considering only a subset of the available predictor variables for each split in the tree helps ensure the trees a different.

We end up exploring the data from many different angles.

---

## Random forests

Random forests are examples of **ensemble methods**.

They average predictions across multiple different trees to give a better prediction than any one tree.

There are two key parameters when fitting a random forest that can potentially be tuned:

1. The number of predictor variables to consider for each split.

    - Using fewer predictor variables for each split can ensure the trees differ.

2. The tree depth. Trees are not pruned - while individual trees over-fit, we average out the over-fitting.

    - Controlled by restricting splits to nodes with some minimum number of observations.

---

.left-code[
## Random forest: Wage data

```{r, message=FALSE, warning=FALSE}
library(randomForest)
set.seed(1945)
wage.rf.1 <- randomForest(WAGE ~ .,
               data = wage.train)
wage.rf.1.pred <- wage.test %>%
  bind_cols(
    .pred=predict(wage.rf.1,
                  newdata=wage.test)
    )
wage.rf.1.pred %>%
  summarise(
    MSE = mean((.pred - WAGE)^2))
```

The forest does well without any tuning.

The first two trees are very deep and very different.
]

.right-plot[
```{r, echo=FALSE, message=FALSE}
library(reprtree)
tree1 <- reprtree:::as.tree(getTree(wage.rf.1, k=1, labelVar = TRUE),
                            wage.rf.1)
tree2 <- reprtree:::as.tree(getTree(wage.rf.1, k=2, labelVar = TRUE),
                            wage.rf.1)
par(mfrow=c(2,1), mar=c(0,0,1,0))
plot(tree1, type="uniform", main="Tree 1")
text(tree1, cex=0.4, label=NULL)
plot(tree2, type="uniform", main="Tree 2")
text(tree2, cex=0.4, label=NULL)
```
]

---

## Random forest: Tuning

We can tune the number of predictors to consider at each split using `mtry`.

- This defaults to $p/3$ where $p$ is the number of predictors, so in the case of the `WAGE` data will be 3.

The tree depth is controlled by `nodesize`, the minimum size of nodes before a split is allowed.

- This defaults to 5 for prediction.

Tuning these parameters is sometimes useful for improving the model fit.
---

## Random forest: Tuning for the Wage data

```{r}
wage.rf.2 <- randomForest(WAGE ~ ., data=wage.train, mtry = 2)
wage.rf.3 <- randomForest(WAGE ~ ., data=wage.train, mtry = 2, nodesize = 1)
wage.rf.pred <- wage.test %>%
                  bind_cols(
                    .pred1 = predict(wage.rf.1, newdata=wage.test),
                    .pred2 = predict(wage.rf.2, newdata=wage.test),
                    .pred3 = predict(wage.rf.3, newdata=wage.test)
                  )
wage.rf.pred %>%
  summarise(MSE.1 = mean((.pred1 - WAGE)^2),
            MSE.2 = mean((.pred2 - WAGE)^2),
            MSE.3 = mean((.pred3 - WAGE)^2))
```

In this example the original forest does better than the tuned ones.