---
title: 'Lecture 1'
subtitle: 'Data Mining'
output:
  xaringan::moon_reader:
    css: [default, default-fonts, "custom.css"]
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
    nature:
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);" />
        </div>
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(4.8, 4.5), fig.retina=2, out.width="100%", comment="#>")
theme_set(theme_minimal())
```

class: middle, inverse

## Welcome to Data Mining

---

## Data Mining vs Statistics

There's a lot of overlap between many disciplines, and none more so than Statistics, Data Science, Data Mining, Artificial Intelligence and Machine Learning.

And even more broadly with Computer Science, Information Technology and Mathematics. And perhaps even more importantly some of the social sciences, especially ethics.

We'll be doing lots of these things in this course, though will be focusing on the statistical side.

In some ways, the difference is really in the size and type of problems being solved.

---

## Data Mining vs Statistics

- Statisticians tend to start by making modelling assumptions about how the data is generated. Generally these assumptions then give a mathematical framework in which to answer specific questions.

- Machine learning people tend to treat the mechanism that generates the data as unknown (or unknowable) and are happy to use any algorithmic model that gets the job done.

---

.pull-left[
## Example: Goats

Worm infestation in goats can be countered by drenching.

- Designed experiment (conducted in Lincoln, New Zealand) to investigate effect of drenching regimes on goats.

- 40 goats were divided at random into two groups of 20.

- Goats in first group got standard drenching program; those in second got intensive drenching regimen.

- Goat weight (kg) measured at start and end of year.
]

.pull-right[
```{r, echo=FALSE, fig.dim=c(2.5,4), out.width='100%', fig.align='right'}
knitr::include_graphics("graphics/goat.jpg")
```
]

---

.left-code[
## Example: Goats

- Principal question of interest here is whether type of drenching program affects a goat's weight gain.

- Secondary question is whether the level of weight gain depends on the goat's initial weight, and if so in what way.
]

.right-plot[
```{r, echo=FALSE, message=FALSE}
goats <- read_csv("../data/goats.csv")
ggplot(goats) +
  geom_point(aes(x=Wt.ini, y=Wt.gain, shape=Treatment, col=Treatment)) +
  labs(x = "Initial weight", y="Weight gain") +
  scale_colour_manual(values = c("black", "red")) +
  scale_shape_manual(values = c("circle open", "cross")) +
  theme(legend.position = 'bottom')
```
]

---

## A classical statistics problem

- This is kind of dataset and scientific problem that would have been familiar to statisticians almost a century ago.

- The dataset is of modest size.

- $n = 40$ cases (records)

- $p = 3$ variables (weight gain; initial weight; and treatment program)

- A statistician would use standard tools like regression and hypothesis testing.

---

## Example: Data Mining of Amazon Reviews

- Data on reviews published on Amazon.

- Information on 30 reviews by each of 50 reviewers (total of $n = 1500$ reviews).

- For each review, $p = 10,000$ variables are recorded. e.g.

    - Word frequencies (e.g. number of occurrences word 'book' in each review);
    - Usage of digits and punctuation;
    - Word and sentence length

---

```{r, echo=FALSE, message=FALSE, fig.dim=c(8,5), out.width='90%', fig.align='center'}
library(patchwork)
space_comma <- read_csv("../data/amazon_space_comma.csv")
g1 <- ggplot(space_comma) + geom_point(aes(x=Spaces, y=Commas), alpha=0.5) +
  labs(x = "Number of spaces", y = "Number of commas")
g2 <- ggplot(space_comma) + geom_histogram(aes(x=Spaces), bins=20, fill='grey70', col='grey30') + theme_void()
g3 <- ggplot(space_comma) + geom_histogram(aes(y=Commas), bins=20, fill='grey70', col='grey30') + theme_void()

g2 + plot_spacer() + g1 + g3 +
  plot_layout(ncol = 2, 
    nrow = 2, 
    widths = c(4, 1),
    heights = c(1, 4))
```

---

## Example: Data Mining of Amazon Reviews

- Very large dataset: 15 million individual pieces of data (goat data was 120).

- Dataset massively multivariate: $p = 10,000$ variables per record.

    - Note $p = 10000$ is larger than $n = 1500$, which would be unusual in classical statistics.

- Data collected in part for study of stylometry, but no specific scientific hypotheses.

- Data analysis could include:

    - Classification (who wrote what?)
    - Associations in written language (do comma users also like semi-colons?)
    - Applications in linguistics, artificial intelligence, source attribution.

---

.pull-left[
## Classical Statistics

- Datasets  can be quite modest in size.

- Methods typically designed to deal with situations where there are many more records $n$ than variables $p$.

- The aim of an analysis is often estimation or testing of some parameter or another.

- Models are usually easy to interpret.
]

.pull-right[
## Data Mining

- Almost always involves large to massive datasets.

- Having more variables $p$ than records $n$ is fairly common.

- The problems are more often concerned with classification, prediction, or association.

- Models are typically complex and 'black box' in nature.
]

---

## Key steps in data mining

1. Collect data (or get given it).

2. **Wrangle the data into shape.**

3. Train models (the more the better!)

4. Choose the best model.

5. Use the best model for prediction.

In practice, we end up spending the most time in step 2, as usually the data we find or are provided with needs work before we get on to fitting models.

---

class: middle, inverse

# Data wrangling

---

## Data wrangling

In general, data wrangling consists of doing everything necessary to get datasets 'tidy' and ready for modelling. This might involve:

- Transforming the shape of the data (e.g. from wide to long or vice-versa).

- Joining datasets from multiple sources.

- Processing columns to extract meaningful information.

- Combining columns of data to produce new columns.

- Filtering data (rows or columns) to remove junk we don't want.

- Dealing with missing values.

- Removing things that humans have added that aren't useful for computers.

We'll start the course with a quick review of how to do some of these using `dplyr` and `tidyr`, among other packages.

---

## Data wrangling with `dplyr`

Recall that `dplyr` has a set of verbs that help us solve the most common data manipulation challenges:

- `select` to choose variables (columns) by name.
- `filter` to choose observations (rows) by value.
- `mutate` to add new variables based of existing variables.
- `summarise` to reduce multiple values down to a single summary.
- `arrange` changes the order of rows.

All of these combine with `group_by` to do all the above operation on subgroups of the data.

---

## `dplyr`

One of the nice things about `dplyr` is it abstracts the language of data processing across the implementation of where the data exist. It works with `data.frame`'s loaded into memory, as well as remote databases (via SQL, or Apache Spark) using the same set of R commands.

The other nice thing is the consistency:
 - All verbs take in as their first argument a `data.frame`.
 - Then all other arguments describe what to do with the `data.frame`.
 - The result is then a new `data.frame`.

```{r, eval=FALSE}
new_data <- do_something(old_data, other, stuff)
```

This means that it works with the pipe `|>` (or `%>%` from the `magrittr` package). i.e.

```{r, eval=FALSE}
new_data <- old_data |> do_something(other, stuff)
```

---

## Filtering rows
 
Let's look at some data from Horizon's Regional Council on river quality.

```{r, include=FALSE}
library(lubridate)
hrc <- read_csv("https://www.massey.ac.nz/~jcmarsha/data/horizons_river_ecoli.csv")
date <- hrc |> mutate(Year = year(ymd(Date))) |> pull(Year) |> range()
```

```{r, message=FALSE}
hrc <- read_csv("https://www.massey.ac.nz/~jcmarsha/data/horizons_river_ecoli.csv")
hrc
```

We have *E. coli* counts from `r hrc |> count(Site) |> nrow()` sites throughout the region from `r date[1]` to `r date[2]`.

---

## Filtering rows
 
What if we just wanted the rows from 2017 onwards?
 
```{r}
hrc |> filter(Date >= '2017-01-01')
```

---

## Filtering rows
 
Or rows with large counts?

```{r}
hrc |> filter(Value > 100000)
```

---

## Filtering rows
 
What if we wanted really low counts before the end of 2010?

```{r}
hrc |> filter(Value < 100, Date <= '2010-12-31')
```

---

## Filtering rows

What if we wanted the extreme low or high counts?

```{r}
hrc |> filter(Value < 3 | Value > 100000)
```

---

## Logical expressions in R

The `filter` argument is a logical expression. i.e. it's something that evaluates to `TRUE` or `FALSE`.

For convenience, filter can take more than one logical expression, separated by a comma. In this case they're AND'd together (i.e. rows must satisfy both criteria), but anything within each expression is done first.

If we want to use OR rather than AND, we use the vertical bar `|` character.

i.e. `hrc |> filter(Value < 3 | Value > 100000, SiteID == "00081")`

will return rows where the value is very small or very large from Site 00081.

---

## Logical expressions in R

Other operations are `!=` (not equal to) and `%in%` (in). e.g.

```{r}
hrc |> filter(Value %in% c(3,5))
```

---

## Arranging rows

Rather than pick rows out, we can sort the data instead.

```{r}
hrc |> arrange(Value)
```

---

## Arranging rows

Selecting more than one variable to arrange on will first arrange on the first, then use later variables for ties:

```{r}
hrc |> arrange(Value, Date)
```

---

## Arranging rows

To arrange descending rather than ascending, we can use `desc`

```{r}
hrc |> arrange(Value, desc(Date))
```

Interestingly, no value of 1 after 2012... This suggests a change in the way things were measured.

---

## Combining filter and arrange

The nice thing about the pipe is it allows us to chain things together. Let's look at post-2012 values:

```{r}
hrc |>
  filter(Date >= '2013-01-01') |>
  arrange(Value)
```

It seems some of the values recorded as 4 actually mean "less than 4".

---

## Combining filter and arrange

```{r}
hrc |> filter(Value > 50000) |> arrange(Site, Date)
```

---

## Selecting and renaming columns

Just like we can select rows with `filter`, we can choose columns with `select`.

This is useful for getting rid of columns we don't need, for rearranging columns, and for changing names to
something more convenient.

```{r}
hrc |> select(Site, Date, Value)
```

---

## Selecting columns

We can select a range of columns with a colon `:`

```{r}
hrc |> select(Council, Site:RawValue)
```

---

## Selecting columns

The `everything()` helper function is useful if you want one column first, then everything else:

```{r}
hrc |> select(Symbol, Value, everything())
```

---

## Selecting columns

The `starts_with()` helper is useful if you have a bunch of columns that start similarly:

```{r}
hrc |> select(Date, Value, starts_with("Site"))
```

---

## Dropping columns

To drop a column use `-` in front of the name:

```{r}
hrc |> select(-Indicator, -RawValue, -Symbol)
```

---

## Renaming columns

If you want to rename a column while keeping the other columns, then `rename()` does this:

```{r}
hrc |> rename(Name = Site)
```

---

## Adding new columns

We can add new columns based on existing ones with `mutate`:

```{r}
hrc |> select(SiteID, Date, Value) |>
  mutate(Log10Value = log10(Value))
```

---

## Adding new columns

We can add new columns based on existing ones with `mutate`:

```{r, message=FALSE}
library(lubridate)
hrc |> select(SiteID, Date, Value) |>
  mutate(Year = year(Date), Month = month(Date, label=TRUE))
```

---

.left-code[
## Seasonality?
```{r, label='hrc_season', eval=FALSE}
hrc |>
  mutate(
    Month = month(Date, label=TRUE)
    ) |>
  ggplot() +
  geom_boxplot(
    aes(
      x = Month,
      y = Value)
    ) +
  scale_y_log10(
    labels = scales::label_comma()
  )
```

We can pipe the result of wrangling into
`ggplot()` as it takes a data.frame as first
argument.

Remember to switch from `|>` to `+` !!
]

.right-plot[
```{r, ref.label='hrc_season', echo=FALSE, eval=TRUE}
```
]

---

## Changing columns

We can change existing columns by overwriting them:

```{r}
hrc |> select(SiteID, Site, Date, Value) |>
  mutate(SiteID = as.numeric(SiteID),
         Site = str_to_title(Site))
```

---

## Summarising rows

`summarise` computes summary statistics across all rows.

It generally takes lots of rows, performs some computation on one (or more) columns, and results in a single row output.

e.g. you can use this to compute means, medians, counts, totals or other summary statistics:

```{r}
hrc |> summarise(Mean = mean(Value))
```

---

## Summarising rows

You can do as many summaries at once as you like:

```{r}
hrc |> summarise(Mean = mean(Value),
                  Median = median(Value),
                  IQR = IQR(Value),
                  Count = n(),
                  Total = sum(Value),
                  Mean2 = Total / Count)
```

Notice in the `Mean2` computation, we're using columns that we computed earlier in the same `summarise()` command.

The `n()` function is useful for number of rows. Also `n_distinct()` for the number of distinct values.

---

## Grouping

The power of `dplyr` is that it can do all of the previous actions on subgroups of the data (all at once) rather than the entire data.

.pull-left[
```{r}
hrc |> select(SiteID, Date, Value)
```
]

.pull-right[
```{r}
hrc |> select(SiteID, Date, Value) |> 
  group_by(SiteID)
```
]

---

.pull-left[

## Grouping

So while this produces a single row:

```{r}
hrc |>
  summarise(Median = median(Value))
```
]

--

.pull-right[
## &nbsp;

This produces multiple rows, one for each year:

```{r}
hrc |> mutate(Year = year(Date)) |>
  group_by(Year) |> 
  summarise(Median = median(Value))
```
]

---

## Grouping multiple variables

When you `group_by()` more than one variable, the other verbs operate within unique combinations of those variables.

If you do a `summarise()` then you reduce the data down to one row for the inner-most variable. So it will drop this level of grouping, retaining grouping at higher levels.

```{r}
hrc |>
  mutate(Year = year(Date)) |>
  group_by(Site, Year) |>
  summarise(Median = median(Value))
```

---

This means that subsequent operations happen at the `Site` level:

```{r}
hrc |>
  mutate(Year = year(Date)) |>
  group_by(Site, Year) |>
  summarise(Median = median(Value)) |>
  mutate(Overall = median(Median))
```

---

You can remove grouping with `ungroup()`:

```{r}
hrc |>
  mutate(Year = year(Date)) |>
  group_by(Site, Year) |>
  summarise(Median = median(Value)) |>
  ungroup() |>
  mutate(Overall = median(Median))
```

---

## Counting

There are a number of other helper functions in `dplyr` thare are useful to know, such as `count()`.

```{r, eval=FALSE}
hrc |> count(Site)
```

is equivalent to:

```{r, eval=FALSE}
hrc |> group_by(Site) |> summarise(n = n())
```

The function `add_count()` adds a count column instead of summarising.

---

.pull-left[
```{r}
hrc |> count(Site)
```
]
.pull-right[
```{r}
hrc |> add_count(Site) |>
  select(Site, Value, n)
```
]

---

## Other helpers

`slice_min()` and `slice_max()` are useful for finding the top (or bottom) few entries.

.pull-left[
```{r}
hrc |> select(Site, Value) |>
  arrange(desc(Value)) |>
  filter(row_number() <= 10)
```
]

.pull-right[
```{r}
hrc |>
  select(Site, Value) |>
  slice_max(Value, n=10)
```
]

---

## Other helpers

There are also `slice()` for taking rows out by row number, or `slice_head()` and `slice_tail()` for taking from the ends. `slice_sample()` can be used to take a random sample.

```{r}
hrc |> slice_sample(n=10)
```

---

## Putting it all together

With the 6 main functions `select`, `arrange`, `filter`, `mutate`, `summarise` and `group_by` we can now answer many questions:

- Which are the best or worst sites on average (e.g. by Median)?

- Which sites do we not have much data on?

- Which sites are new?

- Which sites are tested infrequently?

---

Which sites are the **best** on average (by median)

```{r}
hrc |> group_by(Site) |>
  summarise(Median = median(Value)) |>
  arrange(Median)
```

---

Which sites are the **worst** on average (by median)

```{r}
hrc |> group_by(Site) |>
  summarise(Median = median(Value)) |>
  arrange(desc(Median))
```

---

Which sites are the **best** or **worst** on average (by median)

.pull-left[
```{r, message=FALSE}
hrc |> group_by(Site) |>
  summarise(Median = median(Value)) |>
  slice_min(Median, n=5)
```
]

.pull-right[
```{r, message=FALSE}
hrc |> group_by(Site) |>
  summarise(Median = median(Value)) |>
  slice_max(Median, n=5)
```
]

---

Which sites do we not have much data on?

```{r}
hrc |> count(Site) |> arrange(n)
```

---

Which sites are new?

```{r}
hrc |> group_by(Site) |>
  summarise(FirstDate = min(Date)) |>
  arrange(desc(FirstDate))
```

---

Which sites are tested infrequently?

```{r}
hrc |> group_by(Site) |>
  summarise(DateRange = max(Date) - min(Date), Count = n()) |>
  mutate(DaysPerTest = DateRange / Count) |>
  arrange(desc(DaysPerTest))
```
