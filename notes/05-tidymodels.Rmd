# Tidy modelling

```{r, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(comment = "#>")
theme_set(theme_minimal())
options(pillar.sigfig = 5)
```

The [prediction chapter](#prediction) has shown us how to fit and assess the performance
of the [linear model](#sec:regress), [regression trees](#sec:regtree), [random forests](#sec:randomforest) and [neural networks](#sec:neuralnet).

In each case both the model fitting and obtaining predictions was consistent:

- We use a formula syntax to specify the model and provide the training data.
- We use the `predict` command on the resulting model object, providing testing data.

This works well as long as all our models follow this same syntax. Unfortunately,
many models require different syntaxes for one or both of these steps.

In addition, there are often other things that we may want to do, such as
processing the data beforehand (e.g. to scale predictors to a common scale or impute
missing values). Any data transformation we perform on the training set will
need to be replicated on the test set. Further, the tools we've used for
summarising model performance (computing MSE via `dplyr::summarise()`) are metric-specific,
and it would be nice to be able to easily switch to other metrics, or to compute
many metrics at once (e.g. the mean absolute error, or root mean squared error).

The `tidymodels` set of packages is designed for modelling in a consistent and tidy
fashion. The idea is that it abstracts the specific syntaxes for
fitting different models and provides a common interface that generally works across
a wide range of model types. It also has packages for data processing and model
evaluation, among other things.

We will be looking at just part of the `tidymodels` framework, and will be leaving
some of the more powerful aspects (e.g. workflows and model tuning) aside.

The packages we'll be focusing on in this chapter are:

- `rsample` for resampling.
- `recipes` for data processing.
- `parsnip` for model fitting.
- `yardstick` for measuring model performance.

The [tidymodels website](https://tidymodels.org) has a number of excellent tutorials and an overview of some of the additional packages available (e.g. `tune`, `workshop`).

## Splitting and resampling data

The `rsample` package is used for resample data sets. A dataset resample is where the dataset is split in two in some way. This may be via *bootstrapping*, sampling the same number of rows from a data set with replacement, or by randomly dividing the rows into two sets, creating training and validation or artificial test sets. In either case the rows of the data are in one of two sets: Either in the 'analysis' or 'training' set, or the 'assessment', 'testing' or 'validation' sets[^bootstraprows].

This process may be repeated multiple times, producing multiple pairs of analysis and assessment sets. In the case of bootstrapping, the analysis sets could be used to compute a resampling distribution of some summary statistic such as the sample mean[^bootstrapresample]. In the case of a data split, it might be for cross-validation. Collections of resamples like this can be managed using the `rsample` package.

The main functionality that we'll be using is to split a data set into training set and validation sets using the `initial_split()` function, so that we can compare model performance to select the best model for the dataset at hand. We can then use that best model, retrained on all the data to build a final model for predicting the outcome on a separate testing dataset where the outcome is unknown.

::: {.example #wagesplit}
**Splitting the US Wage data**

We can split the US wage data into a new training and artificial test/validation set using the `initial_split()` function.

```{r, message=FALSE, warning=FALSE}
library(rsample)
set.seed(1945)
wage.orig <- read_csv("../data/wage-train.csv") |> rowid_to_column()
split <- initial_split(wage.orig, prop = 0.75)
split
wage.new.train <- training(split)
wage.validation <- testing(split)
wage.new.train |> slice_head(n=4)
wage.validation |> slice_head(n=4)
```

 -  For demonstration purposes, we add row identifiers to the original data set using `rowid_to_column()`.
 
 -  We supply the proportion that we wish to remain in the training/analysis portion of the data to
    `initial_split()`. The default is 0.75 which is a good rule of thumb.

 -  The object returned by `initial_split` is of type `rsplit`, holding details of the data split
    (the underlying dataset and the rows to be assigned to either set).

 -  The `training()` and `testing()` functions then extract the relevant dataset from the split.
 
 -  Looking at the first few rows of each shows how the rows have been randomly assigned to each data set.
:::

## Data processing with recipes

Often the data that we have to work with needs processing prior to applying models. This may be due to data quality issues (e.g. we may have to impute missing values or remove columns or rows that are very incomplete) or may be to ensure the data is in the form for a particular model (e.g. neural networks require numeric variables to be reasonably small and on similar scales to avoid computational issues, and other models require factor variables to be converted to numeric via indicator variables).

In the case of transforming variables via scaling, such as normalising a numeric measure to have mean zero and variance one, we need to ensure that the transformation is identical across the training and testing datasets. We can't normalise variables across the two sets separately, as the datasets are likely to have at least slightly different means and variances.

The `recipes` package takes care of these details, providing a set of instructions (a `recipe`) for taking our raw data (ingredients) and preparing a data set ready for modelling.

::: {.example #wagerecipe}
**A recipe for scaling the US Wage data**

Suppose we wish to prepare the new training and validation sets from example \@ref(exm:wagesplit) for use in a neural network model, so wish to normalise the numeric variables to the same scale (mean 0, variance 1)[^normalwage]. We do this by constructing a `recipe` for this data, specifying the formula and dataset we plan to use for modelling which defines outcome variable and predictors:

```{r, message=FALSE, warning=FALSE}
library(recipes)
wage.recipe <- recipe(WAGE ~ ., data=wage.new.train)
wage.recipe
```

Our next step is to remove the `rowid` variable as not being useful for modelling and to normalise the numeric variables except those that are binary indicators (`UNION` and `SOUTH`):

```{r}
wage.recipe.norm <- wage.recipe |>
  step_rm(rowid) |>
  step_normalize(all_numeric_predictors(), -UNION, -SOUTH)
wage.recipe.norm
```
We then prep (train) this recipe with the training set:

```{r}
wage.recipe.trained <- wage.recipe.norm |> prep(wage.new.train)
wage.recipe.trained
```

Finally, we `bake` our recipe into the final datasets:

```{r}
wage.train.baked <- wage.recipe.trained |> bake(wage.new.train)
wage.valid.baked <- wage.recipe.trained |> bake(wage.validation)
```

This has resulted in datasets where our numeric variables have been centered and scaled to have mean zero and standard deviation one:

```{r, render = knitr::normal_print}
skimr::skim(wage.train.baked)
```
Note that the baked validation set will not have mean zero and standard deviation one, as it was normalised using the same transformation as used for the training set (i.e. subtract the mean and divide by the standard deviation from the training set). Nonetheless they should be expected to be close to zero and one:

```{r, render = knitr::normal_print}
skimr::skim(wage.valid.baked)
```

Typically the above process will be done without many of the intermediate objects[^baking]:

```{r}
wage.recipe <- recipe(WAGE ~ ., data=wage.new.train) |>
  step_rm(rowid) |>
  step_normalize(all_numeric_predictors(), -UNION, -SOUTH)
wage.prepped <- wage.recipe |> prep(wage.new.train)
wage.train.baked <- bake(wage.prepped, wage.new.train)
wage.valid.baked <- bake(wage.prepped, wage.validation)
```
:::

## Modelling with parnsip

We have seen throughout the [prediction chapter](#prediction) that each prediction model has it's own functions available for fitting the model and predicting new observations given a model.

In the case of linear models via `lm`, regression trees via `rpart`, random forests via `randomForest`and neural networks via `nnet` both the construction of models and prediction from those models follow essentially identical syntaxes. We fit models using:

```{r, eval=FALSE}
model <- model_fit_function(outcome ~ predictors, data = training)
```

and do prediction via:

```{r, eval=FALSE}
pred <- predict(model, newdata=testing)
```

Unfortunately, this convenient formulation is not the norm. There are model types that are fit by providing a vector outcome and matrix of predictors (e.g. `glmnet` for penalised regression), and there are model types where predictions require additional processing to yield a vector of outcomes (e.g. `ranger` for random forests).

The `parnsip` package[^parnsippun] is designed to alleviate this by providing a consistent interface to a wide range of models for both prediction and classification. The idea is that we have essentially two main functions `fit` and `predict` that apply over different types of model fit using different modelling engines. For example, the linear regression model `linear_reg` can be fit using the `lm` engine we're familiar with, or via penalised regression[^penalised] with `glmnet` or in a Bayesian framework[^bayes] with `stan`. Whichever model or engine we choose, the `predict` method will always return a data frame with one row for each row of the new data supplied to it which is convenient for binding as a new column.

::: {.example #wagefit}
**Fitting a linear regression and neural network to the US Wage data using parsnip**

Below we utilise the baked training set from Example \@ref(exm:wagerecipe) to
fit a linear regression and neural network model with the `parsnip` package.

```{r}
library(parsnip)
set.seed(1937)
wage.spec.lm <- linear_reg(mode = "regression",
                           engine = "lm")
wage.spec.nn <- mlp(mode = "regression",
                    engine="nnet",
                    hidden_units = 5,
                    penalty = 0.01,
                    epochs = 1000)

wage.fit.lm <- wage.spec.lm |> fit(WAGE ~ ., data=wage.train.baked)
wage.fit.nn <- wage.spec.nn |> fit(WAGE ~ ., data=wage.train.baked)

wage.fit.lm |> extract_fit_engine() |> summary()
```
Some notes:

 -  As we'll be fitting a neural network, we set the random seed so we can reproduce these results.

 -  The `linear_reg` command details that we're performing regression using the `lm` engine.
    These are the defaults, so we could have just used `linear_reg()` here.

 -  The `mlp` command is short for MultiLayer Perceptron model (a neural network). We're configuring
    the model in regression mode (as neural nets can also do classification) using the `nnet` engine
    which is the default. The additional parameter `hidden_units` defines the number of nodes in the
    hidden layer (i.e. the `size` argument of `nnet`) while `penalty` defines the amount of weight decay
    (the `decay` parameter of `nnet`). The `epochs` parameter is for the maximum number of training
    iterations (`maxit` in `nnet`). The `parsnip` package redefines it's own names for these
    parameters and translates them for each of the possible engines that can be used (e.g. neural nets
    can also be fit using the `keras` engine).

 -  Once we've defined the model specifications, we then fit each model via `fit()`, supplying the
    model formula and training data.

 -  The objects returned from `fit()` are of type `model_fit`, which is a wrapper around whichever
    object type was returned from the given engine. We can use `extract_fit_engine()` to pull out
    the actual model object (e.g. the result from `lm` in the case of linear regression). This can
    be useful for interogating the model fit such as looking at coefficients or plotting a regression
    tree.

Once we have our model fits, we can perform prediction:
 
```{r}
wage.pred <- wage.validation |>
  bind_cols(
    predict(wage.fit.lm, new_data = wage.valid.baked) |> rename(pred.lm = .pred),
    predict(wage.fit.nn, new_data = wage.valid.baked) |> rename(pred.nn = .pred)
    )

wage.pred |>
  select(rowid, WAGE, pred.lm, pred.nn, everything()) |>
  slice_head(n=4)

wage.pred |> summarise(
  MSE.lm = mean((WAGE - pred.lm)^2),
  MSE.nn = mean((WAGE - pred.nn)^2)
)
```

 -  In the `predict()` functions we're providing the baked validation data so that we match the baked 
    training data.

 -  We choose to bind the predictions to the original validation data, rather than the baked
    validation data. This can be useful for comparing predictions with predictors on the original
    scales.

 -  The `predict()` function returns a data frame with a column `.pred` containing the predictions. As
    we have two models here and wish to have the predictions side by side, we're renaming each of these
    to something we will remember.

 -  As we have a data.frame from the prediction, we can easily summarise these results to extract the MSE.
    In this case, the linear regression performs better than the neural network.
:::

## Model performance with yardstick

We have largely been using the mean square error for assessing model predictive performance. This is a useful measure in that it generally decomposes to be the sum of the variance and bias squared, so captures the variance-bias trade-off well. However, as it is a squared measure, it is not on the same scale as the outcome variable, which makes translating the MSE into it's effect on individual predictions difficult.

An alternate measure would be the **root mean squared error** (RMSE), the square root of MSE. This is on the same scale as the data, so at least the magnitude can be used to assess predictive performance. If the MSE is 25, then this corresponds to an RMSE of 5, so we might expect predictions to generally be within about 10 units (twice the RMSE - in the same way a 95% confidence interval corresponds to roughly 2 standard errors) of their true value, assuming the errors are relatively symmetric.

But there are other measures available as well. The **mean absolute error** is given by
$$\textsf{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|,$$
which is also on the same scale as the outcome variable. This measure is preferred in the case where there may be just a few observations that are predicted poorly. Such observations contribute greatly to the MSE or RMSE as the squaring of already large errors amplifies their effects. These observations will still contribute more than other observations to the MAE, but there is no squaring to further accentuate their effect.

The `yardstick` package provides a range of metrics for assessing model performance.

::: {.example}
**Assessing model fits on the US Wage data with yardstick**

Consider the models fit in Example \@ref(exm:wagefit). The `metrics` function from `yardstick` gives a set of standard metrics (RMSE, $R^2$, MAE) for predictive performance on each model in turn.

```{r, message=FALSE}
library(yardstick)
wage.pred |> metrics(truth = WAGE, estimate = pred.lm)
wage.pred |> metrics(truth = WAGE, estimate = pred.nn)
```

Alternatively, we could pivot our two model predictions long so we can evaluate both
models at once.

```{r}
wage.long <- wage.pred |>
  pivot_longer(starts_with("pred."), names_to="model", values_to="pred")

wage.long |>
  group_by(model) |>
  metrics(truth = WAGE, estimate = pred)
```
Some notes:

 -  The two columns `pred.lm` and `pred.nn` contain our predictions. We need the predictions
    in a single column if we wish to evaluate both at once. The `pivot_longer()` specification
    creates a `model` column filled with "pred.lm" and "pred.nn", and a `pred` column with the
    corresponding predictions.

 -  We `group_by(model)` so that the `metrics` function is run on each model in turn.

 -  The result allows relatively easy comparison, though we could optionally add
 
    ```{r, eval=FALSE}
    pivot_wider(names_from = model, values_from = .estimate)
    ```
    
    if we wanted a three by two table.
    
 -  In this example the linear model does better than the neural network across all metrics.

The other advantage to pivoting longer is we can more easily produce charts of the predictions, such as that given in Figure \@ref(fig:wagepredictions). It is clear from this chart that the neural network is performing considerably worse than the linear model.

```{r wagepredictions, fig.cap="Comparison of predictions for the US Wage data from a linear model and neural network"}
ggplot(data = wage.long) +
  geom_point(mapping = aes(x=WAGE, y=pred, col=model), alpha=0.7) +
  scale_colour_manual(values = c(pred.lm = 'black', pred.nn='red')) +
  labs(x = "WAGE (truth)", y = "WAGE (predicted)")
```

:::

[^bootstraprows]: In the case of a bootstrap resample, rows may appear multiple times in the analysis set, as they are sampled with replacement. The rows in the assessment set are rows that don't appear in the analysis set.

[^bootstrapresample]: The great thing about the bootstrap, is that it can be applied to just about any summary statistic. e.g. you could fit a complicated model to each analysis set and extract the resampling distribution of any parameter from the model.

[^normalwage]: The numeric variables in the US wage are not on vastly different scales, so this may be less important for this example than for other datasets.

[^baking]: Often, rather than creating the final baked datasets, the baking is done directly in the modelling stage by supplying `bake(recipe, data)` directly to the modelling or predict functions.

[^parnsippun]: The name `parsnip` is a play on words. It is a reference to the older `caret` package (short for classification and regression training) which has a similar goal, but is implemented very differently from a technical perspective.

[^penalised]: In a penalised regression the coefficients $\beta_k$ are found by minimising $$RSS + \lambda \sum_{k=1}^p \beta_k^2.$$ Setting $\lambda=0$ results in the conventional estimates of $\beta_k$ that `lm` would provide, while increasing $\lambda > 0$ results in optimal $\beta_k$'s that are closer to 0. This is particularly useful in the case where we have more predictors than we do observations, where the classical estimator would be underspecified. The penalisation parameter $\lambda$ is often found by cross-validation.

[^bayes]: In a Bayesian framework we provide probability distributions that capture our prior beliefs about which values are plausible for the parameters of the model. The data (likelihood) is then used to update these beliefs to reflect the information provided by the data.
